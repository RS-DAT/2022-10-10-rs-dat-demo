{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dcachefs\n",
    "import fsspec\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from xrspatial.classify import natural_breaks\n",
    "\n",
    "from cgc.coclustering import Coclustering\n",
    "from cgc.utils import calculate_cocluster_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-clustering the first-leaf spring index over the conterminous United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Overview\n",
    "\n",
    "This notebook illustrates how to use [Clustering Geo-data Cubes (CGC)](https://cgc.readthedocs.io) to perform the co-clustering analysis of a gridded spring-index model that represents the day of first leaf appearance in the conterminous United States for 42 years. The spring index model has been generated using [this notebook](https://github.com/RS-DAT/JupyterDask-Examples/blob/main/03-phenology/notebooks/02-compute-spring-index.ipynb) following the procedure described in [Izquierdo-Veriguier et al., 2018](https://doi.org/10.1016/j.agrformet.2018.06.028) and it is stored on the [SURF dCache storage](http://doc.grid.surfsara.nl/en/stable/Pages/Service/system_specifications/dcache_specs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Co-clustering \n",
    "\n",
    "The use of co-clustering overcomes some of the limitations of traditional one-dimensional clustering techniques (such as *k*-means). For spatio-temporal data, traditional clustering  allows the identification of spatial patterns across the full time span of the data, or temporal patterns for the whole spatial domain under consideration. With co-clustering, the space and time dimensions are simultaneously clustered. This allows the creation of groups of elements that behave similarly in both dimensions. These groups are called co-clusters and typically represent a region of the study area that has similar temporal dynamics for a subset of the study period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The dataset\n",
    "\n",
    "The data employed in this notebook consists of a spring-index model calculated for the conterminous United States (CONUS) from 1980 to 2021 on a 1-km grid. The data set, which is provided in [Zarr](https://zarr.readthedocs.io/en/stable/) format, includes two spring indices representing the day of the year of first leaf appearance and the day of first bloom. These indices have been computed starting from weather and climate variables from the [Daymet](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1840) dataset.\n",
    "\n",
    "For more information about the data have a look at the original publication: [Izquierdo-Veriguier et al., 2018](https://doi.org/10.1016/j.agrformet.2018.06.028)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Before running this notebook\n",
    "\n",
    "The input dataset is stored on the SURF dCache system, which we access via bearer-token authentication with a macaroon. The macaroon, generated using [this script](https://github.com/sara-nl/GridScripts/blob/master/get-macaroon), is stored together with other configuration parameters within a JSON fsspec configuration file (also see the [STAC2dCache tutorial](https://github.com/NLeSC-GO-common-infrastructure/stac2dcache/blob/main/notebooks/tutorial.ipynb) and the [fsspec documentation](https://filesystem-spec.readthedocs.io/en/latest/features.html#configuration) for more info):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"dcache\": {\n",
    "        \"token\": \"<MACAROON_STRING_HERE>\",\n",
    "        \"api_url\": \"https://dcacheview.grid.surfsara.nl:22880/api/v1\",\n",
    "        \"webdav_url\": \"https://webdav.grid.surfsara.nl:2880\",\n",
    "        \"block_size\": 0, \n",
    "    \"request_kwargs\": {\n",
    "            \"timeout\": 3600\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the dataset\n",
    "\n",
    "### 2.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the spring index dataset in the distributed memory of a [Dask](https://www.dask.org) cluster and reshape the data so that it is suitable for running the co-clustering analysis. We thus create a combined axis from the \"x\" and \"y\" dimensions of the dataset, leading to a two-dimensional (\"time\", \"space\") array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Input parameters\n",
    "\n",
    "We define the range of years to include in the co-clustering analysis and the path to the Zarr root directory where the dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1980, 2022))\n",
    "\n",
    "urlpath = (\n",
    "    \"dcache://pnfs/grid.sara.nl/data/remotesensing/disk/spring-index-models.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Opening the dataset\n",
    "\n",
    "We open the Zarr archive containing the dataset, merging the yearly data in a single array. We open the dataset using the same chunks as employed in the Zarr store. We then select the plant-averaged \"first-leaf\" spring index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_map = fsspec.get_mapper(urlpath, block_size=10*2**20)\n",
    "\n",
    "dss = [\n",
    "    xr.open_zarr(fs_map, group=year, decode_coords=\"all\", chunks=\"auto\")\n",
    "    for year in years\n",
    "]\n",
    "\n",
    "ds = xr.concat(dss, dim=\"time\")\n",
    "ds = ds.assign_coords(time=years)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_index = ds[\"first-leaf\"].sel(plant=\"mean\")\n",
    "spring_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spring index is represented as a three-dimensional (time, y, x) array. We can inspect the data set by plotting a slice along the time dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select years from 1990 to 1993\n",
    "spring_index.sel(time=slice(1990, 1993)).plot.imshow(col=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manipulate the array spatial dimensions creating a combined (x, y) axis. We also drop the grid cells that have null values for any of the years and rechunk the dataset in order to have data chunks that are homogeneous in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_index = spring_index.stack(space=[\"x\", \"y\"])\n",
    "location = np.arange(spring_index.space.size) # create a combined (x,y) index\n",
    "spring_index = spring_index.assign_coords(location=(\"space\", location))\n",
    "\n",
    "# drop pixels that are null-valued for any of the time indices\n",
    "spring_index = spring_index.dropna(\"space\", how=\"any\")  \n",
    "\n",
    "# increase the chunk size along the time dimension\n",
    "spring_index = spring_index.chunk({\"time\": -1, \"space\": 500_000})\n",
    "spring_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of the matrix\n",
    "print(\"{} MB\".format(spring_index.nbytes/2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is correctly reshaped we setup a Dask cluster (e.g. via the Dask JupyterLab extension) and create a client connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--DROP DASK `SLURMCluster` HERE--*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a cluster with 15 nodes. When the nodes are ready, we load the data in the distributed memory of the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=15)\n",
    "spring_index = spring_index.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The co-clustering analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Overview\n",
    "\n",
    "Once we have loaded the data set as a 2D array, we can run the co-clustering analysis. We use the Jenks natural breaks algorithm on the time-averaged spring index to identify suitable initial conditions for the space clusters (the time clusters are instead randomly initialized). The algorithm implemented in CGC iteratively updates the co-clusters until the loss function that corresponds to the information loss in each cluster does not significantly change in two consecutive iterations (a threshold is provided). The solution reached by the algorithm does not necessarily represent the global minimum of the loss function (it might be a local minimum). Thus, multiple differently-initialized runs need to be performed in order to sample the cluster space, and the cluster assignment with the lowest loss-function value selected as best candidate solution. For more information about CGC, have a look at the package [documentation](https://cgc.readthedocs.io/en/latest/user_manual.html#co-clustering) or at [this article](https://doi.org/10.21105/joss.04032)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Input parameters and configuration\n",
    "\n",
    "To run the co-clustering analysis for the first-leaf spring index that we have loaded in the previous section, we first have to choose an initial number of spatial and temporal clusters and set the values of few other parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_clusters = 4\n",
    "num_space_clusters = 10\n",
    "\n",
    "max_iterations = 5  # maximum number of iterations\n",
    "conv_threshold = 1.  # convergence threshold \n",
    "nruns = 100  # number of differently-initialized runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGC makes use of `logging`, we set the desired verbosity via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initial conditions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial conditions can significantly affect the convergence of the co-clustering analysis. Here we use the [Jenks natural breaks](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)  classification methods on a sample of the time-averaged spring index to compute an initial cluster assignment in the spatial domain. Initial conditions for the time clusters are not provided, thus CGC initializes these randomly. For this reason, multiple runs with different (time) cluster initialization need to be computed in order to make sure we get as close as possible to the global optimum.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time-averaged spring index\n",
    "spring_index_mean = spring_index.mean(dim=\"time\")\n",
    "spring_index_mean = spring_index_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute initial space cluster labels using natural breaks\n",
    "nb_labels = natural_breaks(\n",
    "    spring_index_mean.unstack(\"space\"), \n",
    "    num_sample=10_000, \n",
    "    k=num_space_clusters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot initial space cluster labels\n",
    "nb_labels.plot.imshow(x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as for the dataset, create combined x,y dim and drop NaN's\n",
    "space_clusters_init = nb_labels \\\n",
    "    .stack(space=[\"x\", \"y\"]) \\\n",
    "    .dropna(\"space\", how=\"any\") \\\n",
    "    .astype(\"int\") \n",
    "\n",
    "# set the same chunking as for the dataset\n",
    "space_clusters_init = space_clusters_init.chunk({\"space\": 500_000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initial conditions in the distributed memory\n",
    "space_clusters_init = space_clusters_init.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Running the analysis\n",
    "\n",
    "We are now ready to run the analysis. CGC offers multiple implementations of the same co-clustering algorithm. Ultimately, the most suitable implementation for a given problem is determined by the size of the data set of interest and by the infrastructure that is available for the analysis. Here we make use of a Dask-based co-clustering implementation that exploits parallelization for both the data chunks and the co-clustering runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing a `Coclustering` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Coclustering(\n",
    "    spring_index.data,  # data array (must be 2D)\n",
    "    num_time_clusters,  # number of row clusters\n",
    "    num_space_clusters,  # number of column clusters \n",
    "    max_iterations=max_iterations,  \n",
    "    conv_threshold=conv_threshold, \n",
    "    col_clusters_init=space_clusters_init.data,\n",
    "    nruns=nruns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the co-clustering runs, we now pass the instance of the client connection to the Dask cluster to the `run_with_dask` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = cc.run_with_dask(client=client, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by the `run_with_dask` method contains all the co-clustering results, the most relevant being the row (temporal) and column (spatial) cluster assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Row (time) clusters: {results.row_clusters}\")\n",
    "print(f\"Column (space) clusters: {results.col_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(results.col_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integers contained in these arrays represent the cluster labels for each of the two dimensions. They allow to identify the co-cluster to which each element belongs: the (i, j) element of the spring index matrix belongs to the co-cluster (m, n), where m and n are the i-th element of `results.row_clusters` and j-th element of `results.col_clusters`, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate plotting, we create `DataArray`'s for the spatial and temporal cluster labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_clusters = xr.DataArray(results.row_clusters, \n",
    "                             dims='time', \n",
    "                             coords=spring_index.time.coords, \n",
    "                             name='time cluster')\n",
    "space_clusters = xr.DataArray(results.col_clusters, \n",
    "                              dims='space', \n",
    "                              coords=spring_index.space.coords, \n",
    "                              name='space cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the temporal clusters to which each year belongs, and make a histogram of the number of years in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# line plot\n",
    "time_clusters.plot(ax=ax[0], x='time', marker='o')\n",
    "ax[0].set_yticks(range(num_time_clusters))\n",
    "\n",
    "# temporal cluster histogram\n",
    "time_clusters.plot.hist(ax=ax[1], bins=num_time_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial clusters can also be visualized after 'unstacking' the location index that we have initially created, thus reverting to the original (x, y) coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_clusters_xy = space_clusters.unstack('space')\n",
    "space_clusters_xy.plot.imshow(\n",
    "    x='x', y='y', levels=range(num_space_clusters+1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average spring index value of each co-cluster can be computed via a dedicated utility function in CGC, which return the cluster means in a 2D-array with dimensions `(n_row_clusters, n_col_clusters)`. We calculate the cluster averages and create a `DataArray` for further manipulation and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the co-cluster averages\n",
    "means = calculate_cocluster_averages(\n",
    "    spring_index.data, \n",
    "    time_clusters, \n",
    "    space_clusters, \n",
    "    num_time_clusters, \n",
    "    num_space_clusters\n",
    ")\n",
    "means = xr.DataArray(\n",
    "    means,\n",
    "    coords=(\n",
    "        ('time_clusters', range(num_time_clusters)), \n",
    "        ('space_clusters', range(num_space_clusters))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop co-clusters that are not populated \n",
    "means = means.dropna('time_clusters', how='all')\n",
    "means = means.dropna('space_clusters', how='all')\n",
    "\n",
    "# use first row- and column-clusters to sort \n",
    "space_cluster_order = means.isel(time_clusters=0)\n",
    "time_cluster_order = means.isel(space_clusters=0)\n",
    "means_sorted = means.sortby(\n",
    "    [space_cluster_order, time_cluster_order]\n",
    ")\n",
    "# drop labels, no longer needed\n",
    "means_sorted = means_sorted.drop_vars(['time_clusters', 'space_clusters'])\n",
    "\n",
    "# now plot the sorted co-cluster means \n",
    "means_sorted.plot.imshow(\n",
    "    xticks=[], yticks=[],\n",
    "    robust=True,\n",
    "    aspect=4,\n",
    "    size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed co-cluster means and the spatial clusters can be employed to plot the average first-leaf value for each of the temporal clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_means = means.sel(space_clusters=space_clusters, drop=True)\n",
    "space_means = space_means.unstack('space')\n",
    "space_means.plot.imshow(\n",
    "    x='x', y='y', col='time_clusters',\n",
    "    robust=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are done with calculations, we shutdown the cluster to release resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
